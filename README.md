# Comprehensive Literature Review on Large Language Models in EEG Signal Decoding

This table summarizes recent research papers that utilize large language models (LLMs) in EEG signal decoding for various tasks. It includes information about their contributions, datasets, applications, performance, publication venues, and code availability.

| Paper Title | Year | Main Contribution | Dataset | Task/Application | Metrics Used | Performance | GPU Used | Journal/Publication Venue | Code Availability |
|-------------|------|--------------------|---------|------------------|--------------|-------------|----------|---------------------------|--------------------|
| [Decoding EEG Brain Activity for Multi-Modal Natural Language Processing](https://www.frontiersin.org/articles/10.3389/fnhum.2021.659410/full) | 2021 | Investigated the integration of EEG signals with textual data to enhance NLP tasks, demonstrating that EEG features can improve sentiment classification. | Custom dataset with EEG recordings during reading tasks | Sentiment classification | Accuracy | Improved accuracy over baselines; specific metrics not provided | Not specified | *Frontiers in Human Neuroscience* | Not available |
| [Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs)](https://arxiv.org/abs/2410.07507) | 2024 | Proposed a three-stage approach to generate text from EEG signals by fine-tuning LLMs with EEG data, enabling direct text generation from brain activity. | Public EEG dataset with image stimuli | Text generation | BLEU, ROUGE, human evaluation | Demonstrated efficacy in generating coherent text from EEG signals | Not specified | *arXiv Preprint* | [Available](https://github.com/abhijitmishra/Thought2Text) |
| [BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction](https://arxiv.org/abs/2410.14971) | 2024 | Introduced a multi-stage strategy for semantic brain signal decoding via vector-quantized spectrogram reconstruction, enhancing text generation from EEG. | EEG dataset (Brennan) and MEG dataset (GWilliams) | Semantic decoding, text generation | Accuracy, BLEU, ROUGE | Outperformed state-of-the-art methods on both EEG and MEG datasets | Not specified | *arXiv Preprint* | Not available |
| [Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder](https://arxiv.org/abs/2402.17433) | 2024 | Developed a Contrastive EEG-Text Masked Autoencoder (CET-MAE) to enhance EEG-to-text decoding by leveraging pre-trained LLMs for improved language reconstruction. | ZuCo dataset with text-evoked EEG recordings | EEG-to-text decoding | ROUGE-1 F1, BLEU-4 | Outperformed baseline framework by 8.34% in ROUGE-1 F1 and 32.21% in BLEU-4 | Not specified | *arXiv Preprint* | Not available |
| [BELT: Bootstrapped EEG-to-Language Training by Natural Language Supervision](https://arxiv.org/abs/2309.12056) | 2023 | Introduced BELT, a model that bootstraps EEG representation learning using large LMs, improving brain-to-language translation and zero-shot sentiment classification. | EEG datasets with natural reading tasks | Brain-to-language translation, sentiment classification | BLEU-1, precision | Achieved 42.31% BLEU-1 score and 67.32% precision in sentiment classification | Not specified | *arXiv Preprint* | Not available |
| [EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation](https://arxiv.org/abs/2401.18006) | 2024 | Proposed EEG-GPT, leveraging LLMs for EEG classification, achieving performance comparable to state-of-the-art methods with enhanced interpretability. | EEG datasets with normal and abnormal recordings | EEG classification | Accuracy | Achieved excellent performance in classifying normal vs. abnormal EEG with limited training data | Not specified | *arXiv Preprint* | Not available |
| [Deep Representation Learning for Open Vocabulary Electroencephalography-to-Text Decoding](https://arxiv.org/abs/2312.09430) | 2023 | Presented an end-to-end deep learning framework for open vocabulary EEG-to-text decoding, incorporating subject-dependent representation learning and a BART language model. | ZuCo v1.0 and v2.0 datasets with EEG recordings during natural reading tasks | EEG-to-text decoding | BLEU-1, ROUGE-1-F, BERTScore-F | Achieved 42.75% BLEU-1, 33.28% ROUGE-1-F, and 53.86% BERTScore-F, outperforming previous state-of-the-art methods | Not specified | *arXiv Preprint* | Not available |
| [Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI](https://arxiv.org/abs/2405.18765) | 2024 | Proposed LaBraM, a unified foundation model for EEG that enables cross-dataset learning through unsupervised pre-training, improving performance across multiple EEG tasks. | Approximately 2,500 hours of various EEG signals from around 20 datasets | Abnormal detection, event type classification, emotion recognition, gait prediction | Various task-specific metrics | Outperformed state-of-the-art methods in respective fields | Not specified | *arXiv Preprint* | [Available](https://github.com/935963004/LaBraM) |
| [Neuro-GPT: Towards A Foundation Model for EEG](https://arxiv.org/abs/2311.03764) | 2023 | Introduced Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT model, pre-trained on large-scale data to improve performance in low-data regimes for EEG tasks. | Large-scale EEG dataset | Motor imagery classification | Classification accuracy | Significantly improved classification performance compared to models trained from scratch | Not specified | *arXiv Preprint* | [Available](https://github.com/wenhui0206/NeuroGPT) |

## Notes:
- The GPU models used for training are not specified in most papers.
- For more details, visit the provided links to the respective papers and repositories.
